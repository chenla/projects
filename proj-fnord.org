#   -*- mode: org; fill-column: 60 -*-
#+TITLE: Fnord
#+STARTUP: showall
#+PROPERTY: filename
  :PROPERTIES:
  :Name: /home/deerpig/proj/chenla/projects/proj-fnord.org
  :Created: 2017-03-23T18:58@Prek Leap (11.642600N-104.919210W)
  :ID: 5e84bb05-427b-4056-b225-9ce659018c54
  :URL:
  :END:
#+INCLUDE: ./inc/head.org
#+INCLUDE: ./inc/macros.org

{{{breadcrumbs}}}

#+HTML: <div class="outline-2" id="meta">
| *Author*  | Brad Collins <brad@chenla.org>             |
| *Date*    | {{{time(%Y-%m-%d %H:%M:%S)}}}              |
#+HTML: </div>

#+TOC: headlines 4


* Status of This Document

The latest version of this document is: src_sh[:results value]{git describe --abbrev=0 --tags}

#+HTML: <div class="notice notice-info">
*Changes since the last version*

#+HTML: <ul>

#+BEGIN_SRC sh :exports results :results value html :var TAG="v00.01.02"
  RES='git log $TAG...HEAD --pretty="<li>%s</li>\n" -- (file-name-nondirectory (buffer-file-name))'
  if [ -z $RES ]; then
    RES="<li>no changes</li>"
  fi
  echo $RES
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT html
<li>no changes</li>
#+END_EXPORT

#+HTML: </ul>
#+HTML: </div>

* Introduction

The bitcoin block chain is about establishing facts, but a very narrow
type of truth; the assertion that a piece of information has been
transfered between two parties, that constitutes a digital transaction.

But there are many different types of assertions that can not be
determined to be true or not that require a number of different
approaches.  In effect we need a toolbox of different tools that can
be used in different combinations to do this.

Why is this important?  In a word, survival.

We survive as individuals, groups and as a species by observing the
world, making assertions about the world -- determine how likely they
are true or not, and then make decsions based on those assertions.  If
we are right, we survive.  If we make too many mistakes or even get a
single existential assertion wrong -- we die.

Our survival depends on our ability to take in information, make
assertions, determine their likelihood fast enough to stay alive.  We
evolved to be able to do this on the Savannah.  We formed into groups,
to increase the amount of information that could be taken in and
evaluated and then shared that information to increase our chances of
survival.  It worked, but in doing so we generated more information
and complexity that required more assertions and mechanisms for the
group to be able to make collective assessments.

Mankind's history can be written as a chain of innovations that we
have made to increase our ability to collect, assert, evaluate and
share information.  And each time we innovate the more complexity we
generate and then have to innovate again to keep up.  This happened at
a fairly steady slow pace for milennia.  But gradually, we began to
realise that the innovation cycles were growing shorter with each new
innovation.  This is in part because all of these innovations made it
possible for us to live longer and more productive lives, and for
infant mortality to be reduced to no longer being statistically
important to our survival.  The population grew dramatically, but this
was not simply a side effect of all of this innovation, it was needed
in order to have enough people around that could innovate fast enough
to keep up with the ever shorter innovation cycles.  This is what
Alvin Toffler was talking about when saying that the rate of change is
increasing logarithmically.  Simply put, we can not go back to a
simplier past without a corresponding die-off in the population.

This does not mean that population will continue to grow forever as
long as we can keep up with the innovation cycle.  By reducing infant
mortality and educating women we have effectively put the brakes on
out of control population growth.  The United Nations, along with most
of the scientific community see global population continue to grow to
just under 10 billion, followed by a massive shrinking of the
population -- possibly as low as half of today's global population.

In many respects this is good news, but what remains to be seen is how
this will effect the innovation cycle.  Man has managed to use up most
of the easily accessible natural energy and raw material resources.
And we have gone so far beyond the carrying capacity of the planet
without those innovations, that the resulting problems are going to
start rapidly accelerating.  Even if we return to a population that is
half of today's, the damage done to the climate, and the exhaustion of
resources means that the innovation cycle will have to continue to
accelerate for many more generations until the climate can return to
the stability that we have enjoyed over the past hundreds of years.

The problem is that we won't have the population to be able to
innovate as quickly, let alone fast enough to keep ahead of these
problems long enough to get through the next several hundred years.

Clearly, what this means is that we will need to find a way of making
each member of that smaller, stable population to be orders of
magnitude more cognitively productive than we do now.

There are a number of different ways this can happen, or more
accurately, we will need combination of different things:

  - cognitive surplus :: clay shirky -- need to harness a larger
       percentage of our population to innovate and put the cognitive
       surplus that now is spent on time sinks like Facebook to
       actually do things that improve our ability to survive -- not
       just wallow in a glut of products from the present
       manufacturing surplus.
  - cognitive extension :: AI is about extending our neocortex, AI is
       us and we are AI.
  - extending muscle :: in a nutshell, robotics 
  - expansion :: it will be impossible to do if mankind is confined to
       this planet, we need to become a solar speicies and
       civilization -- innovation requires new places to
       expand into.  That's what the moon, mars and the belt
       are about.  if we don't expand we die, it's that
       simple.  Expansion gives us breathing room, it gives us places
       to let all of the smart AI driven robots to exploit resources
       it allows us to move heavy industry and the pollution that is 
       the driver of climate change to places where it won't matter. 
       Space can't be more toxic and dangerous than it already is.

And at the heart of all of this -- the fundemental building block for
all of this to happen, or not, is for us to find a way of making
assertations, evaluating them and coming to a consensus about their
validity.

We need this as individuals trying to cope with information overload.
We need this as families, as companies, as governments.  It needs to
be foundational utility as ubiquitious as tap water and electricity.

These assertions, once vertified, and evaluated become facts, facts
that will be used to power our self driving cars, shopping,
manufacturing, commerce, banking and currency, politics and
government, healthcare, education, agriculture and science.

So what are the functional requirements for this global network of
facts? 

   - scalable -- must work on the smallest scales up to the largest,
     from kilobytes to exabytes and larger.
   - geographically scalable -- must work publically across whole
     solar system as well as being intimately personal and private.
   - temporally scalable -- designed to work over very short and
     survive very long time scales
   - distributed & federated -- no one owns it, no one and everyone
     controls it.
   - everyone participates -- no walled gardens
   - everyone has access -- no walled gardens
   - both generated dynamically and composed -- 
   - transparent & trustless -- 
   - both machine and human readable -- 


Before we can design a platform that can do all of this and more, we
need to be a bit more clear about the atomic unit behind the /fact/
network.  And the first thing we have to let go of is the idea of a
fact.  Let's take for an example two assertions that we can reasonably
claim to be facts about the poet Emily Dickinson:

   - Emily Dickinson was born on December 10, 1830.
   - Emily Dickinson died on May 15, 1886.

So today you can say that Emily Dickinson is dead. But that would not
be true in 1831, and wouldn't be true until 1886.  Truth is no
eternal, it depends on LOD and POV that provide context that makes the
assertion true or false.

** Possible Causal Chains

There is another class of truth and falsity as well, it must be within
what is possible -- there has to be a causal chain in which that
assertion could be true.  This is important in order for us to limit
facts to scenarios that can happen, no matter how improbable.  This
makes is possible to construct a map from the territory of all
scenarios which /is/ infinite.  Limiting assertions to what is
possible, even if that is as fine-grained as binary quantum events,
the number of possible combinations is not infinite.  And this applies
to fictional universes as well -- that facts can be established in any
internally consistent universe, be it real or imagined.

** Assertions are not enough

Simply making an assertion does not make it true.  Given
that an assertion must be possible... Assertions can be true
or false.  They can also be more or less certain and more or
less probable.  And because all of these things are at least
in part dependent on establishing a causal-chain, and
surrounding context of when an assertion is made, or
observed, we need a term that embodies all of this at the
same time.

We will call such assertions a /fnord/.  Fnords are a
probability cloud of all possible states of an assertion, so
that the assertion may be true or false, probable or
improbable, certain and uncertain all at the same tie until
it has been observed.  When it is observed it becomes one of
those states.  But it might have a different state when
observed in a different context.

** Graphs need fnords

An RDF triple, with a subject, object and predicate is an
assertion.  If you take a bunch of these assertions and link
them together you get a graph database.  Graph databases are
very powerful ways of creating models of data.  But in the
end each triple is an assertion that may or may not be
possible or true or how certain it might be.

In fact, in RDF, just as is true in XML (HTML is an instance
of XML), everything is a string.  Element names are strings,
they have no defined, or established meaning.  In HTML <p>
is the paragraph element.  But there is nothing in HTML that
establishes what a paragraph is.  Even schema languages
can't tell us what <strong> or <em> or <h1> is.  This is
only explained using a prose definition in the HTML
specification which is not machine understandable.  This is
far more problematic than it might seem at first glance.

The <p> element makes a cultural assumption that the concept
of a paragraph is understood by everyone.  Such assumed
knowledge is needed for communication to be possible.  But
such assumed knowledge changes over time.  And again,
machines are not privy to that shared knowledge.  So
undefined labels and assertions are only of limited use.

** Homoiconigraphy

One of the problems with encoding assertions as data, is
that they can't embody the range of possibilities that are
needed to encode a fnord -- fnords, by definition can not be
static, because they depend on an observer.  Recorded data
is not able to dynamically change according the LOD and POV
of the observer when observing a fnord.  For this we need
executable code.  This is very similar to our own brains, we
don't remember things like a camera, we recreate memories
which change over time as new memories are added.  This
gives the brain a very flexible means of making and
assessing fnords.  Something that has been recorded on paper
or in a photograph can't do that, they are snapshots, frozen
moments that are, by definition, no longer true the second
that they have been recorded because the surrounding context
has changed.  In other words, fnords have a dual nature,
that can be seen as data when observed one way, and as code
when observed in another.  The equivilance of code and data
is called homoiconigraphy.

The neo-cortex can be thought of as a graph-database that
is as much code as it is data.


** is this a viable business?

Verisign which today has a 2017 market cap of nearly $9B was built on
a very small single aspect of what fnord will be able to offer

in fact because fnords are distributed -- there will be many different
multi-billion dollar companies that can be based on this platform

the scale we are talking about is the same magnitude as the entire
world wide web -- it is that big.

actually it will be larger -- because every contract, every software
application, every ride in a taxi, every hair drier, coffee machine,
fridge (as well as everything in it), every jumbo jet, every
government budget, or company audit, civil or criminal court case,
every piece of land and kilowat of electricity consumed, every door
opened, or thermostat set will have fnords associated with them.

fnords have the potential of being the glue that holds our
civilization together, a trustless ubiquitious meta institution that
makes our survival possible. 
